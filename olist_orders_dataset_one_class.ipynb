{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8436795002ee83fa",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f7d22eb00a5650e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T18:53:07.931843Z",
     "start_time": "2023-11-14T18:53:07.814191Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('temp/olist_orders_dataset_df.csv')\n",
    "X = df.drop(['anomaly'], axis=1)\n",
    "y = df['anomaly']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f4035a78a751e07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T18:53:07.935132Z",
     "start_time": "2023-11-14T18:53:07.932866Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102280, 18)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3831d4bd8de414cf",
   "metadata": {},
   "source": [
    "### Normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "97a38e3cced99b1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T18:53:08.211847Z",
     "start_time": "2023-11-14T18:53:07.936760Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "qt = QuantileTransformer(output_distribution='normal')\n",
    "X = qt.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2c2cdfcab70c7d",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "607ee949bbe2b3b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T18:53:08.218545Z",
     "start_time": "2023-11-14T18:53:08.213578Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a674c9635851dfa8",
   "metadata": {},
   "source": [
    "### Import Necessary Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37547022c5180827",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T18:53:08.221527Z",
     "start_time": "2023-11-14T18:53:08.219975Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c137046b772254c3",
   "metadata": {},
   "source": [
    "### Define the Autoencoder Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca17addda7452a14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T18:53:08.224681Z",
     "start_time": "2023-11-14T18:53:08.221898Z"
    }
   },
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Encoder layers\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(18, 14),  # Assuming the first hidden layer has 12 nodes\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(14, 10),   # Further compressing to 8 nodes\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 6)\n",
    "        )\n",
    "        # Decoder layers\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(6, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 14),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(14, 18),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e50a8a6077a603",
   "metadata": {},
   "source": [
    "### Instantiate the Model, Loss Function, and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d09e3d44caaf908a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T18:53:08.228760Z",
     "start_time": "2023-11-14T18:53:08.226022Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Autoencoder()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271bd124c7d246d8",
   "metadata": {},
   "source": [
    "### Convert DataFrame to PyTorch Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d3c0c08732c41353",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T18:53:08.231625Z",
     "start_time": "2023-11-14T18:53:08.229123Z"
    }
   },
   "outputs": [],
   "source": [
    "tensor_data = torch.Tensor(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be889862a7e06fe",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c2b5d16c32481064",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T18:53:08.241353Z",
     "start_time": "2023-11-14T18:53:08.231789Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "dataset = TensorDataset(tensor_data)\n",
    "train_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54ac6f11d45422d",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2b27ccb22bc30335",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T18:57:55.131062Z",
     "start_time": "2023-11-14T18:53:08.233505Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300], Loss: 0.0177\n",
      "Epoch [2/300], Loss: 0.0107\n",
      "Epoch [3/300], Loss: 0.0068\n",
      "Epoch [4/300], Loss: 0.0111\n",
      "Epoch [5/300], Loss: 0.0156\n",
      "Epoch [6/300], Loss: 0.0070\n",
      "Epoch [7/300], Loss: 0.0099\n",
      "Epoch [8/300], Loss: 0.0118\n",
      "Epoch [9/300], Loss: 0.0089\n",
      "Epoch [10/300], Loss: 0.0092\n",
      "Epoch [11/300], Loss: 0.0065\n",
      "Epoch [12/300], Loss: 0.0080\n",
      "Epoch [13/300], Loss: 0.0077\n",
      "Epoch [14/300], Loss: 0.0095\n",
      "Epoch [15/300], Loss: 0.0052\n",
      "Epoch [16/300], Loss: 0.0067\n",
      "Epoch [17/300], Loss: 0.0105\n",
      "Epoch [18/300], Loss: 0.0089\n",
      "Epoch [19/300], Loss: 0.0062\n",
      "Epoch [20/300], Loss: 0.0085\n",
      "Epoch [21/300], Loss: 0.0059\n",
      "Epoch [22/300], Loss: 0.0075\n",
      "Epoch [23/300], Loss: 0.0066\n",
      "Epoch [24/300], Loss: 0.0079\n",
      "Epoch [25/300], Loss: 0.0089\n",
      "Epoch [26/300], Loss: 0.0065\n",
      "Epoch [27/300], Loss: 0.0086\n",
      "Epoch [28/300], Loss: 0.0053\n",
      "Epoch [29/300], Loss: 0.0085\n",
      "Epoch [30/300], Loss: 0.0094\n",
      "Epoch [31/300], Loss: 0.0056\n",
      "Epoch [32/300], Loss: 0.0057\n",
      "Epoch [33/300], Loss: 0.0032\n",
      "Epoch [34/300], Loss: 0.0074\n",
      "Epoch [35/300], Loss: 0.0051\n",
      "Epoch [36/300], Loss: 0.0060\n",
      "Epoch [37/300], Loss: 0.0063\n",
      "Epoch [38/300], Loss: 0.0071\n",
      "Epoch [39/300], Loss: 0.0051\n",
      "Epoch [40/300], Loss: 0.0072\n",
      "Epoch [41/300], Loss: 0.0087\n",
      "Epoch [42/300], Loss: 0.0066\n",
      "Epoch [43/300], Loss: 0.0069\n",
      "Epoch [44/300], Loss: 0.0129\n",
      "Epoch [45/300], Loss: 0.0070\n",
      "Epoch [46/300], Loss: 0.0088\n",
      "Epoch [47/300], Loss: 0.0077\n",
      "Epoch [48/300], Loss: 0.0050\n",
      "Epoch [49/300], Loss: 0.0050\n",
      "Epoch [50/300], Loss: 0.0077\n",
      "Epoch [51/300], Loss: 0.0060\n",
      "Epoch [52/300], Loss: 0.0059\n",
      "Epoch [53/300], Loss: 0.0080\n",
      "Epoch [54/300], Loss: 0.0043\n",
      "Epoch [55/300], Loss: 0.0042\n",
      "Epoch [56/300], Loss: 0.0069\n",
      "Epoch [57/300], Loss: 0.0088\n",
      "Epoch [58/300], Loss: 0.0035\n",
      "Epoch [59/300], Loss: 0.0048\n",
      "Epoch [60/300], Loss: 0.0063\n",
      "Epoch [61/300], Loss: 0.0053\n",
      "Epoch [62/300], Loss: 0.0072\n",
      "Epoch [63/300], Loss: 0.0046\n",
      "Epoch [64/300], Loss: 0.0073\n",
      "Epoch [65/300], Loss: 0.0058\n",
      "Epoch [66/300], Loss: 0.0058\n",
      "Epoch [67/300], Loss: 0.0040\n",
      "Epoch [68/300], Loss: 0.0049\n",
      "Epoch [69/300], Loss: 0.0040\n",
      "Epoch [70/300], Loss: 0.0068\n",
      "Epoch [71/300], Loss: 0.0047\n",
      "Epoch [72/300], Loss: 0.0049\n",
      "Epoch [73/300], Loss: 0.0046\n",
      "Epoch [74/300], Loss: 0.0048\n",
      "Epoch [75/300], Loss: 0.0055\n",
      "Epoch [76/300], Loss: 0.0085\n",
      "Epoch [77/300], Loss: 0.0049\n",
      "Epoch [78/300], Loss: 0.0074\n",
      "Epoch [79/300], Loss: 0.0052\n",
      "Epoch [80/300], Loss: 0.0060\n",
      "Epoch [81/300], Loss: 0.0041\n",
      "Epoch [82/300], Loss: 0.0043\n",
      "Epoch [83/300], Loss: 0.0050\n",
      "Epoch [84/300], Loss: 0.0063\n",
      "Epoch [85/300], Loss: 0.0051\n",
      "Epoch [86/300], Loss: 0.0061\n",
      "Epoch [87/300], Loss: 0.0050\n",
      "Epoch [88/300], Loss: 0.0046\n",
      "Epoch [89/300], Loss: 0.0053\n",
      "Epoch [90/300], Loss: 0.0070\n",
      "Epoch [91/300], Loss: 0.0054\n",
      "Epoch [92/300], Loss: 0.0064\n",
      "Epoch [93/300], Loss: 0.0066\n",
      "Epoch [94/300], Loss: 0.0061\n",
      "Epoch [95/300], Loss: 0.0088\n",
      "Epoch [96/300], Loss: 0.0047\n",
      "Epoch [97/300], Loss: 0.0036\n",
      "Epoch [98/300], Loss: 0.0044\n",
      "Epoch [99/300], Loss: 0.0065\n",
      "Epoch [100/300], Loss: 0.0056\n",
      "Epoch [101/300], Loss: 0.0051\n",
      "Epoch [102/300], Loss: 0.0039\n",
      "Epoch [103/300], Loss: 0.0056\n",
      "Epoch [104/300], Loss: 0.0064\n",
      "Epoch [105/300], Loss: 0.0045\n",
      "Epoch [106/300], Loss: 0.0064\n",
      "Epoch [107/300], Loss: 0.0067\n",
      "Epoch [108/300], Loss: 0.0044\n",
      "Epoch [109/300], Loss: 0.0062\n",
      "Epoch [110/300], Loss: 0.0037\n",
      "Epoch [111/300], Loss: 0.0053\n",
      "Epoch [112/300], Loss: 0.0054\n",
      "Epoch [113/300], Loss: 0.0058\n",
      "Epoch [114/300], Loss: 0.0040\n",
      "Epoch [115/300], Loss: 0.0044\n",
      "Epoch [116/300], Loss: 0.0058\n",
      "Epoch [117/300], Loss: 0.0048\n",
      "Epoch [118/300], Loss: 0.0049\n",
      "Epoch [119/300], Loss: 0.0069\n",
      "Epoch [120/300], Loss: 0.0075\n",
      "Epoch [121/300], Loss: 0.0052\n",
      "Epoch [122/300], Loss: 0.0048\n",
      "Epoch [123/300], Loss: 0.0068\n",
      "Epoch [124/300], Loss: 0.0072\n",
      "Epoch [125/300], Loss: 0.0048\n",
      "Epoch [126/300], Loss: 0.0055\n",
      "Epoch [127/300], Loss: 0.0047\n",
      "Epoch [128/300], Loss: 0.0053\n",
      "Epoch [129/300], Loss: 0.0049\n",
      "Epoch [130/300], Loss: 0.0071\n",
      "Epoch [131/300], Loss: 0.0061\n",
      "Epoch [132/300], Loss: 0.0045\n",
      "Epoch [133/300], Loss: 0.0042\n",
      "Epoch [134/300], Loss: 0.0025\n",
      "Epoch [135/300], Loss: 0.0033\n",
      "Epoch [136/300], Loss: 0.0043\n",
      "Epoch [137/300], Loss: 0.0042\n",
      "Epoch [138/300], Loss: 0.0073\n",
      "Epoch [139/300], Loss: 0.0049\n",
      "Epoch [140/300], Loss: 0.0066\n",
      "Epoch [141/300], Loss: 0.0044\n",
      "Epoch [142/300], Loss: 0.0042\n",
      "Epoch [143/300], Loss: 0.0054\n",
      "Epoch [144/300], Loss: 0.0043\n",
      "Epoch [145/300], Loss: 0.0045\n",
      "Epoch [146/300], Loss: 0.0053\n",
      "Epoch [147/300], Loss: 0.0044\n",
      "Epoch [148/300], Loss: 0.0053\n",
      "Epoch [149/300], Loss: 0.0058\n",
      "Epoch [150/300], Loss: 0.0038\n",
      "Epoch [151/300], Loss: 0.0047\n",
      "Epoch [152/300], Loss: 0.0039\n",
      "Epoch [153/300], Loss: 0.0059\n",
      "Epoch [154/300], Loss: 0.0042\n",
      "Epoch [155/300], Loss: 0.0031\n",
      "Epoch [156/300], Loss: 0.0055\n",
      "Epoch [157/300], Loss: 0.0047\n",
      "Epoch [158/300], Loss: 0.0048\n",
      "Epoch [159/300], Loss: 0.0043\n",
      "Epoch [160/300], Loss: 0.0043\n",
      "Epoch [161/300], Loss: 0.0048\n",
      "Epoch [162/300], Loss: 0.0052\n",
      "Epoch [163/300], Loss: 0.0051\n",
      "Epoch [164/300], Loss: 0.0049\n",
      "Epoch [165/300], Loss: 0.0060\n",
      "Epoch [166/300], Loss: 0.0050\n",
      "Epoch [167/300], Loss: 0.0067\n",
      "Epoch [168/300], Loss: 0.0066\n",
      "Epoch [169/300], Loss: 0.0052\n",
      "Epoch [170/300], Loss: 0.0049\n",
      "Epoch [171/300], Loss: 0.0047\n",
      "Epoch [172/300], Loss: 0.0040\n",
      "Epoch [173/300], Loss: 0.0029\n",
      "Epoch [174/300], Loss: 0.0039\n",
      "Epoch [175/300], Loss: 0.0055\n",
      "Epoch [176/300], Loss: 0.0047\n",
      "Epoch [177/300], Loss: 0.0034\n",
      "Epoch [178/300], Loss: 0.0049\n",
      "Epoch [179/300], Loss: 0.0026\n",
      "Epoch [180/300], Loss: 0.0055\n",
      "Epoch [181/300], Loss: 0.0070\n",
      "Epoch [182/300], Loss: 0.0043\n",
      "Epoch [183/300], Loss: 0.0041\n",
      "Epoch [184/300], Loss: 0.0031\n",
      "Epoch [185/300], Loss: 0.0066\n",
      "Epoch [186/300], Loss: 0.0040\n",
      "Epoch [187/300], Loss: 0.0054\n",
      "Epoch [188/300], Loss: 0.0041\n",
      "Epoch [189/300], Loss: 0.0053\n",
      "Epoch [190/300], Loss: 0.0041\n",
      "Epoch [191/300], Loss: 0.0055\n",
      "Epoch [192/300], Loss: 0.0030\n",
      "Epoch [193/300], Loss: 0.0065\n",
      "Epoch [194/300], Loss: 0.0051\n",
      "Epoch [195/300], Loss: 0.0062\n",
      "Epoch [196/300], Loss: 0.0046\n",
      "Epoch [197/300], Loss: 0.0040\n",
      "Epoch [198/300], Loss: 0.0038\n",
      "Epoch [199/300], Loss: 0.0044\n",
      "Epoch [200/300], Loss: 0.0044\n",
      "Epoch [201/300], Loss: 0.0051\n",
      "Epoch [202/300], Loss: 0.0044\n",
      "Epoch [203/300], Loss: 0.0047\n",
      "Epoch [204/300], Loss: 0.0054\n",
      "Epoch [205/300], Loss: 0.0034\n",
      "Epoch [206/300], Loss: 0.0064\n",
      "Epoch [207/300], Loss: 0.0033\n",
      "Epoch [208/300], Loss: 0.0047\n",
      "Epoch [209/300], Loss: 0.0055\n",
      "Epoch [210/300], Loss: 0.0039\n",
      "Epoch [211/300], Loss: 0.0039\n",
      "Epoch [212/300], Loss: 0.0048\n",
      "Epoch [213/300], Loss: 0.0045\n",
      "Epoch [214/300], Loss: 0.0046\n",
      "Epoch [215/300], Loss: 0.0035\n",
      "Epoch [216/300], Loss: 0.0048\n",
      "Epoch [217/300], Loss: 0.0040\n",
      "Epoch [218/300], Loss: 0.0046\n",
      "Epoch [219/300], Loss: 0.0048\n",
      "Epoch [220/300], Loss: 0.0043\n",
      "Epoch [221/300], Loss: 0.0044\n",
      "Epoch [222/300], Loss: 0.0075\n",
      "Epoch [223/300], Loss: 0.0048\n",
      "Epoch [224/300], Loss: 0.0031\n",
      "Epoch [225/300], Loss: 0.0053\n",
      "Epoch [226/300], Loss: 0.0073\n",
      "Epoch [227/300], Loss: 0.0042\n",
      "Epoch [228/300], Loss: 0.0043\n",
      "Epoch [229/300], Loss: 0.0066\n",
      "Epoch [230/300], Loss: 0.0050\n",
      "Epoch [231/300], Loss: 0.0037\n",
      "Epoch [232/300], Loss: 0.0051\n",
      "Epoch [233/300], Loss: 0.0031\n",
      "Epoch [234/300], Loss: 0.0036\n",
      "Epoch [235/300], Loss: 0.0046\n",
      "Epoch [236/300], Loss: 0.0037\n",
      "Epoch [237/300], Loss: 0.0038\n",
      "Epoch [238/300], Loss: 0.0023\n",
      "Epoch [239/300], Loss: 0.0040\n",
      "Epoch [240/300], Loss: 0.0036\n",
      "Epoch [241/300], Loss: 0.0037\n",
      "Epoch [242/300], Loss: 0.0039\n",
      "Epoch [243/300], Loss: 0.0060\n",
      "Epoch [244/300], Loss: 0.0071\n",
      "Epoch [245/300], Loss: 0.0035\n",
      "Epoch [246/300], Loss: 0.0044\n",
      "Epoch [247/300], Loss: 0.0049\n",
      "Epoch [248/300], Loss: 0.0035\n",
      "Epoch [249/300], Loss: 0.0040\n",
      "Epoch [250/300], Loss: 0.0048\n",
      "Epoch [251/300], Loss: 0.0041\n",
      "Epoch [252/300], Loss: 0.0043\n",
      "Epoch [253/300], Loss: 0.0037\n",
      "Epoch [254/300], Loss: 0.0050\n",
      "Epoch [255/300], Loss: 0.0041\n",
      "Epoch [256/300], Loss: 0.0053\n",
      "Epoch [257/300], Loss: 0.0043\n",
      "Epoch [258/300], Loss: 0.0052\n",
      "Epoch [259/300], Loss: 0.0041\n",
      "Epoch [260/300], Loss: 0.0047\n",
      "Epoch [261/300], Loss: 0.0042\n",
      "Epoch [262/300], Loss: 0.0048\n",
      "Epoch [263/300], Loss: 0.0034\n",
      "Epoch [264/300], Loss: 0.0031\n",
      "Epoch [265/300], Loss: 0.0048\n",
      "Epoch [266/300], Loss: 0.0050\n",
      "Epoch [267/300], Loss: 0.0043\n",
      "Epoch [268/300], Loss: 0.0030\n",
      "Epoch [269/300], Loss: 0.0035\n",
      "Epoch [270/300], Loss: 0.0045\n",
      "Epoch [271/300], Loss: 0.0056\n",
      "Epoch [272/300], Loss: 0.0046\n",
      "Epoch [273/300], Loss: 0.0072\n",
      "Epoch [274/300], Loss: 0.0043\n",
      "Epoch [275/300], Loss: 0.0045\n",
      "Epoch [276/300], Loss: 0.0050\n",
      "Epoch [277/300], Loss: 0.0036\n",
      "Epoch [278/300], Loss: 0.0042\n",
      "Epoch [279/300], Loss: 0.0050\n",
      "Epoch [280/300], Loss: 0.0057\n",
      "Epoch [281/300], Loss: 0.0073\n",
      "Epoch [282/300], Loss: 0.0041\n",
      "Epoch [283/300], Loss: 0.0058\n",
      "Epoch [284/300], Loss: 0.0038\n",
      "Epoch [285/300], Loss: 0.0037\n",
      "Epoch [286/300], Loss: 0.0035\n",
      "Epoch [287/300], Loss: 0.0053\n",
      "Epoch [288/300], Loss: 0.0038\n",
      "Epoch [289/300], Loss: 0.0046\n",
      "Epoch [290/300], Loss: 0.0033\n",
      "Epoch [291/300], Loss: 0.0048\n",
      "Epoch [292/300], Loss: 0.0054\n",
      "Epoch [293/300], Loss: 0.0052\n",
      "Epoch [294/300], Loss: 0.0035\n",
      "Epoch [295/300], Loss: 0.0064\n",
      "Epoch [296/300], Loss: 0.0028\n",
      "Epoch [297/300], Loss: 0.0055\n",
      "Epoch [298/300], Loss: 0.0040\n",
      "Epoch [299/300], Loss: 0.0040\n",
      "Epoch [300/300], Loss: 0.0047\n"
     ]
    }
   ],
   "source": [
    "# Example training loop\n",
    "num_epochs = 300\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_features, in train_loader:  # DataLoader will unpack the features\n",
    "        # Forward pass\n",
    "        outputs = model(batch_features)\n",
    "        loss = criterion(outputs, batch_features)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a539abb7bae7d2",
   "metadata": {},
   "source": [
    "### Encoding the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a3780189e48e4101",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T18:57:55.137159Z",
     "start_time": "2023-11-14T18:57:55.130091Z"
    }
   },
   "outputs": [],
   "source": [
    "encoded_dataset = model.encoder(tensor_data)\n",
    "encoded_dataset = encoded_dataset.detach().numpy()\n",
    "encoded_dataset = pd.DataFrame(encoded_dataset)\n",
    "\n",
    "encoded_dataset = pd.concat([encoded_dataset, y], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4e5acad024fc6cdb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T18:57:55.391627Z",
     "start_time": "2023-11-14T18:57:55.137714Z"
    }
   },
   "outputs": [],
   "source": [
    "encoded_dataset.to_csv('temp/olist_orders_dataset_autoencoder_df.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1aaf356ad402d61c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T18:57:55.394746Z",
     "start_time": "2023-11-14T18:57:55.391865Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
