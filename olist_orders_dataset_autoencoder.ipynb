{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Loading the dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8436795002ee83fa"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('temp/olist_orders_dataset_df.csv')\n",
    "X = df.drop(['anomaly'], axis=1)\n",
    "\n",
    "columns1 = ['time_estimate_delivery', 'year', 'month', 'day', 'freight_value', 'seller_zip_code_prefix', 'seller_geolocation_lat', 'seller_geolocation_lng','customer_zip_code_prefix', 'customer_geolocation_lat', 'customer_geolocation_lng', 'distance']\n",
    "X = X[columns1]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T18:30:55.483731Z",
     "start_time": "2023-11-13T18:30:55.309251Z"
    }
   },
   "id": "8f7d22eb00a5650e"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "        time_estimate_delivery  year  month  day  freight_value  \\\n0                           14  2017     10    2           8.72   \n1                           18  2018      7   24          22.76   \n2                           27  2018      8    8          19.22   \n3                           23  2017     11   18          27.20   \n4                           12  2018      2   13           8.72   \n...                        ...   ...    ...  ...            ...   \n102275                      23  2018      2    6          20.10   \n102276                      30  2017      8   27          65.02   \n102277                      34  2018      1    8          40.59   \n102278                      34  2018      1    8          40.59   \n102279                      25  2018      3    8          18.36   \n\n        seller_zip_code_prefix  seller_geolocation_lat  \\\n0                       9350.0              -23.680729   \n1                      31570.0              -19.807681   \n2                      14840.0              -21.363502   \n3                      31842.0              -19.837682   \n4                       8752.0              -23.543395   \n...                        ...                     ...   \n102275                 17602.0              -21.930548   \n102276                  8290.0              -23.553642   \n102277                 37175.0              -20.940578   \n102278                 37175.0              -20.940578   \n102279                 14407.0              -20.496251   \n\n        seller_geolocation_lng  customer_zip_code_prefix  \\\n0                   -46.444238                    3149.0   \n1                   -43.980427                   47813.0   \n2                   -48.229601                   75265.0   \n3                   -43.924053                   59296.0   \n4                   -46.262086                    9195.0   \n...                        ...                       ...   \n102275              -50.498348                   11722.0   \n102276              -46.452661                   45920.0   \n102277              -45.827237                   28685.0   \n102278              -45.827237                   28685.0   \n102279              -47.418434                   83750.0   \n\n        customer_geolocation_lat  customer_geolocation_lng      distance  \n0                     -23.576983                -46.587161    662.084641  \n1                     -12.177924                -44.660711   9344.543745  \n2                     -16.745150                -48.514783  10713.151535  \n3                      -5.774190                -35.271143  15422.365811  \n4                     -23.676370                -46.514627    843.950334  \n...                          ...                       ...           ...  \n102275                -24.001500                -46.449864   8679.619783  \n102276                -17.898358                -39.373630   3984.630835  \n102277                -22.562825                -42.694574  16963.820370  \n102278                -22.562825                -42.694574  16963.820370  \n102279                -25.764308                -49.720376   5668.859892  \n\n[102280 rows x 12 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>time_estimate_delivery</th>\n      <th>year</th>\n      <th>month</th>\n      <th>day</th>\n      <th>freight_value</th>\n      <th>seller_zip_code_prefix</th>\n      <th>seller_geolocation_lat</th>\n      <th>seller_geolocation_lng</th>\n      <th>customer_zip_code_prefix</th>\n      <th>customer_geolocation_lat</th>\n      <th>customer_geolocation_lng</th>\n      <th>distance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>14</td>\n      <td>2017</td>\n      <td>10</td>\n      <td>2</td>\n      <td>8.72</td>\n      <td>9350.0</td>\n      <td>-23.680729</td>\n      <td>-46.444238</td>\n      <td>3149.0</td>\n      <td>-23.576983</td>\n      <td>-46.587161</td>\n      <td>662.084641</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>18</td>\n      <td>2018</td>\n      <td>7</td>\n      <td>24</td>\n      <td>22.76</td>\n      <td>31570.0</td>\n      <td>-19.807681</td>\n      <td>-43.980427</td>\n      <td>47813.0</td>\n      <td>-12.177924</td>\n      <td>-44.660711</td>\n      <td>9344.543745</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>27</td>\n      <td>2018</td>\n      <td>8</td>\n      <td>8</td>\n      <td>19.22</td>\n      <td>14840.0</td>\n      <td>-21.363502</td>\n      <td>-48.229601</td>\n      <td>75265.0</td>\n      <td>-16.745150</td>\n      <td>-48.514783</td>\n      <td>10713.151535</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>23</td>\n      <td>2017</td>\n      <td>11</td>\n      <td>18</td>\n      <td>27.20</td>\n      <td>31842.0</td>\n      <td>-19.837682</td>\n      <td>-43.924053</td>\n      <td>59296.0</td>\n      <td>-5.774190</td>\n      <td>-35.271143</td>\n      <td>15422.365811</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>12</td>\n      <td>2018</td>\n      <td>2</td>\n      <td>13</td>\n      <td>8.72</td>\n      <td>8752.0</td>\n      <td>-23.543395</td>\n      <td>-46.262086</td>\n      <td>9195.0</td>\n      <td>-23.676370</td>\n      <td>-46.514627</td>\n      <td>843.950334</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>102275</th>\n      <td>23</td>\n      <td>2018</td>\n      <td>2</td>\n      <td>6</td>\n      <td>20.10</td>\n      <td>17602.0</td>\n      <td>-21.930548</td>\n      <td>-50.498348</td>\n      <td>11722.0</td>\n      <td>-24.001500</td>\n      <td>-46.449864</td>\n      <td>8679.619783</td>\n    </tr>\n    <tr>\n      <th>102276</th>\n      <td>30</td>\n      <td>2017</td>\n      <td>8</td>\n      <td>27</td>\n      <td>65.02</td>\n      <td>8290.0</td>\n      <td>-23.553642</td>\n      <td>-46.452661</td>\n      <td>45920.0</td>\n      <td>-17.898358</td>\n      <td>-39.373630</td>\n      <td>3984.630835</td>\n    </tr>\n    <tr>\n      <th>102277</th>\n      <td>34</td>\n      <td>2018</td>\n      <td>1</td>\n      <td>8</td>\n      <td>40.59</td>\n      <td>37175.0</td>\n      <td>-20.940578</td>\n      <td>-45.827237</td>\n      <td>28685.0</td>\n      <td>-22.562825</td>\n      <td>-42.694574</td>\n      <td>16963.820370</td>\n    </tr>\n    <tr>\n      <th>102278</th>\n      <td>34</td>\n      <td>2018</td>\n      <td>1</td>\n      <td>8</td>\n      <td>40.59</td>\n      <td>37175.0</td>\n      <td>-20.940578</td>\n      <td>-45.827237</td>\n      <td>28685.0</td>\n      <td>-22.562825</td>\n      <td>-42.694574</td>\n      <td>16963.820370</td>\n    </tr>\n    <tr>\n      <th>102279</th>\n      <td>25</td>\n      <td>2018</td>\n      <td>3</td>\n      <td>8</td>\n      <td>18.36</td>\n      <td>14407.0</td>\n      <td>-20.496251</td>\n      <td>-47.418434</td>\n      <td>83750.0</td>\n      <td>-25.764308</td>\n      <td>-49.720376</td>\n      <td>5668.859892</td>\n    </tr>\n  </tbody>\n</table>\n<p>102280 rows Ã— 12 columns</p>\n</div>"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T18:30:55.484169Z",
     "start_time": "2023-11-13T18:30:55.428494Z"
    }
   },
   "id": "9dc7693350132590"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Normalizing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3831d4bd8de414cf"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "qt = QuantileTransformer(output_distribution='normal')\n",
    "X = qt.fit_transform(X)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T18:30:55.613610Z",
     "start_time": "2023-11-13T18:30:55.431740Z"
    }
   },
   "id": "97a38e3cced99b1a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Feature Scaling"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e2c2cdfcab70c7d"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T18:30:55.618944Z",
     "start_time": "2023-11-13T18:30:55.613879Z"
    }
   },
   "id": "607ee949bbe2b3b8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import Necessary Libraries:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a674c9635851dfa8"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T18:30:55.622097Z",
     "start_time": "2023-11-13T18:30:55.619129Z"
    }
   },
   "id": "37547022c5180827"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define the Autoencoder Architecture"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c137046b772254c3"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Encoder layers\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(12, 9),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(9, 6)\n",
    "        )\n",
    "        # Decoder layers\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(6, 9),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(9, 12)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T18:30:55.624835Z",
     "start_time": "2023-11-13T18:30:55.622047Z"
    }
   },
   "id": "ca17addda7452a14"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Instantiate the Model, Loss Function, and Optimizer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4e50a8a6077a603"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "model = Autoencoder()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T18:30:55.627717Z",
     "start_time": "2023-11-13T18:30:55.624201Z"
    }
   },
   "id": "d09e3d44caaf908a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Convert DataFrame to PyTorch Tensor"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "271bd124c7d246d8"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "tensor_data = torch.Tensor(X)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T18:30:55.633039Z",
     "start_time": "2023-11-13T18:30:55.627019Z"
    }
   },
   "id": "d3c0c08732c41353"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### DataLoader"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9be889862a7e06fe"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "dataset = TensorDataset(tensor_data)\n",
    "train_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T18:30:55.633886Z",
     "start_time": "2023-11-13T18:30:55.629540Z"
    }
   },
   "id": "c2b5d16c32481064"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training Loop"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b54ac6f11d45422d"
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Loss: 0.0167\n",
      "Epoch [2/200], Loss: 0.0169\n",
      "Epoch [3/200], Loss: 0.0128\n",
      "Epoch [4/200], Loss: 0.0126\n",
      "Epoch [5/200], Loss: 0.0136\n",
      "Epoch [6/200], Loss: 0.0076\n",
      "Epoch [7/200], Loss: 0.0070\n",
      "Epoch [8/200], Loss: 0.0081\n",
      "Epoch [9/200], Loss: 0.0093\n",
      "Epoch [10/200], Loss: 0.0046\n",
      "Epoch [11/200], Loss: 0.0035\n",
      "Epoch [12/200], Loss: 0.0067\n",
      "Epoch [13/200], Loss: 0.0044\n",
      "Epoch [14/200], Loss: 0.0042\n",
      "Epoch [15/200], Loss: 0.0047\n",
      "Epoch [16/200], Loss: 0.0033\n",
      "Epoch [17/200], Loss: 0.0038\n",
      "Epoch [18/200], Loss: 0.0046\n",
      "Epoch [19/200], Loss: 0.0027\n",
      "Epoch [20/200], Loss: 0.0029\n",
      "Epoch [21/200], Loss: 0.0040\n",
      "Epoch [22/200], Loss: 0.0023\n",
      "Epoch [23/200], Loss: 0.0025\n",
      "Epoch [24/200], Loss: 0.0036\n",
      "Epoch [25/200], Loss: 0.0021\n",
      "Epoch [26/200], Loss: 0.0035\n",
      "Epoch [27/200], Loss: 0.0021\n",
      "Epoch [28/200], Loss: 0.0022\n",
      "Epoch [29/200], Loss: 0.0031\n",
      "Epoch [30/200], Loss: 0.0037\n",
      "Epoch [31/200], Loss: 0.0036\n",
      "Epoch [32/200], Loss: 0.0048\n",
      "Epoch [33/200], Loss: 0.0027\n",
      "Epoch [34/200], Loss: 0.0029\n",
      "Epoch [35/200], Loss: 0.0012\n",
      "Epoch [36/200], Loss: 0.0041\n",
      "Epoch [37/200], Loss: 0.0028\n",
      "Epoch [38/200], Loss: 0.0031\n",
      "Epoch [39/200], Loss: 0.0027\n",
      "Epoch [40/200], Loss: 0.0028\n",
      "Epoch [41/200], Loss: 0.0032\n",
      "Epoch [42/200], Loss: 0.0028\n",
      "Epoch [43/200], Loss: 0.0022\n",
      "Epoch [44/200], Loss: 0.0040\n",
      "Epoch [45/200], Loss: 0.0038\n",
      "Epoch [46/200], Loss: 0.0021\n",
      "Epoch [47/200], Loss: 0.0022\n",
      "Epoch [48/200], Loss: 0.0021\n",
      "Epoch [49/200], Loss: 0.0028\n",
      "Epoch [50/200], Loss: 0.0016\n",
      "Epoch [51/200], Loss: 0.0030\n",
      "Epoch [52/200], Loss: 0.0020\n",
      "Epoch [53/200], Loss: 0.0024\n",
      "Epoch [54/200], Loss: 0.0029\n",
      "Epoch [55/200], Loss: 0.0021\n",
      "Epoch [56/200], Loss: 0.0043\n",
      "Epoch [57/200], Loss: 0.0025\n",
      "Epoch [58/200], Loss: 0.0026\n",
      "Epoch [59/200], Loss: 0.0036\n",
      "Epoch [60/200], Loss: 0.0024\n",
      "Epoch [61/200], Loss: 0.0039\n",
      "Epoch [62/200], Loss: 0.0016\n",
      "Epoch [63/200], Loss: 0.0018\n",
      "Epoch [64/200], Loss: 0.0033\n",
      "Epoch [65/200], Loss: 0.0019\n",
      "Epoch [66/200], Loss: 0.0037\n",
      "Epoch [67/200], Loss: 0.0026\n",
      "Epoch [68/200], Loss: 0.0026\n",
      "Epoch [69/200], Loss: 0.0034\n",
      "Epoch [70/200], Loss: 0.0027\n",
      "Epoch [71/200], Loss: 0.0036\n",
      "Epoch [72/200], Loss: 0.0028\n",
      "Epoch [73/200], Loss: 0.0024\n",
      "Epoch [74/200], Loss: 0.0027\n",
      "Epoch [75/200], Loss: 0.0033\n",
      "Epoch [76/200], Loss: 0.0046\n",
      "Epoch [77/200], Loss: 0.0047\n",
      "Epoch [78/200], Loss: 0.0022\n",
      "Epoch [79/200], Loss: 0.0023\n",
      "Epoch [80/200], Loss: 0.0027\n",
      "Epoch [81/200], Loss: 0.0019\n",
      "Epoch [82/200], Loss: 0.0021\n",
      "Epoch [83/200], Loss: 0.0027\n",
      "Epoch [84/200], Loss: 0.0017\n",
      "Epoch [85/200], Loss: 0.0029\n",
      "Epoch [86/200], Loss: 0.0018\n",
      "Epoch [87/200], Loss: 0.0027\n",
      "Epoch [88/200], Loss: 0.0027\n",
      "Epoch [89/200], Loss: 0.0034\n",
      "Epoch [90/200], Loss: 0.0016\n",
      "Epoch [91/200], Loss: 0.0026\n",
      "Epoch [92/200], Loss: 0.0020\n",
      "Epoch [93/200], Loss: 0.0031\n",
      "Epoch [94/200], Loss: 0.0032\n",
      "Epoch [95/200], Loss: 0.0042\n",
      "Epoch [96/200], Loss: 0.0026\n",
      "Epoch [97/200], Loss: 0.0031\n",
      "Epoch [98/200], Loss: 0.0038\n",
      "Epoch [99/200], Loss: 0.0031\n",
      "Epoch [100/200], Loss: 0.0030\n",
      "Epoch [101/200], Loss: 0.0022\n",
      "Epoch [102/200], Loss: 0.0026\n",
      "Epoch [103/200], Loss: 0.0022\n",
      "Epoch [104/200], Loss: 0.0041\n",
      "Epoch [105/200], Loss: 0.0031\n",
      "Epoch [106/200], Loss: 0.0028\n",
      "Epoch [107/200], Loss: 0.0028\n",
      "Epoch [108/200], Loss: 0.0021\n",
      "Epoch [109/200], Loss: 0.0028\n",
      "Epoch [110/200], Loss: 0.0025\n",
      "Epoch [111/200], Loss: 0.0022\n",
      "Epoch [112/200], Loss: 0.0045\n",
      "Epoch [113/200], Loss: 0.0029\n",
      "Epoch [114/200], Loss: 0.0024\n",
      "Epoch [115/200], Loss: 0.0023\n",
      "Epoch [116/200], Loss: 0.0022\n",
      "Epoch [117/200], Loss: 0.0053\n",
      "Epoch [118/200], Loss: 0.0022\n",
      "Epoch [119/200], Loss: 0.0023\n",
      "Epoch [120/200], Loss: 0.0031\n",
      "Epoch [121/200], Loss: 0.0038\n",
      "Epoch [122/200], Loss: 0.0030\n",
      "Epoch [123/200], Loss: 0.0034\n",
      "Epoch [124/200], Loss: 0.0043\n",
      "Epoch [125/200], Loss: 0.0053\n",
      "Epoch [126/200], Loss: 0.0069\n",
      "Epoch [127/200], Loss: 0.0026\n",
      "Epoch [128/200], Loss: 0.0049\n",
      "Epoch [129/200], Loss: 0.0031\n",
      "Epoch [130/200], Loss: 0.0026\n",
      "Epoch [131/200], Loss: 0.0029\n",
      "Epoch [132/200], Loss: 0.0031\n",
      "Epoch [133/200], Loss: 0.0029\n",
      "Epoch [134/200], Loss: 0.0021\n",
      "Epoch [135/200], Loss: 0.0025\n",
      "Epoch [136/200], Loss: 0.0027\n",
      "Epoch [137/200], Loss: 0.0020\n",
      "Epoch [138/200], Loss: 0.0021\n",
      "Epoch [139/200], Loss: 0.0023\n",
      "Epoch [140/200], Loss: 0.0033\n",
      "Epoch [141/200], Loss: 0.0024\n",
      "Epoch [142/200], Loss: 0.0029\n",
      "Epoch [143/200], Loss: 0.0024\n",
      "Epoch [144/200], Loss: 0.0042\n",
      "Epoch [145/200], Loss: 0.0026\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[40], line 11\u001B[0m\n\u001B[1;32m      9\u001B[0m     \u001B[38;5;66;03m# Backward pass and optimization\u001B[39;00m\n\u001B[1;32m     10\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m---> 11\u001B[0m     loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[1;32m     12\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEpoch [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_epochs\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m], Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mloss\u001B[38;5;241m.\u001B[39mitem()\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.11/site-packages/torch/_tensor.py:505\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    495\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    496\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    497\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    498\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    503\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    504\u001B[0m     )\n\u001B[0;32m--> 505\u001B[0m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mbackward(\n\u001B[1;32m    506\u001B[0m     \u001B[38;5;28mself\u001B[39m, gradient, retain_graph, create_graph, inputs\u001B[38;5;241m=\u001B[39minputs\n\u001B[1;32m    507\u001B[0m )\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    261\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    263\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    264\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    265\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 266\u001B[0m Variable\u001B[38;5;241m.\u001B[39m_execution_engine\u001B[38;5;241m.\u001B[39mrun_backward(  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[1;32m    267\u001B[0m     tensors,\n\u001B[1;32m    268\u001B[0m     grad_tensors_,\n\u001B[1;32m    269\u001B[0m     retain_graph,\n\u001B[1;32m    270\u001B[0m     create_graph,\n\u001B[1;32m    271\u001B[0m     inputs,\n\u001B[1;32m    272\u001B[0m     allow_unreachable\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    273\u001B[0m     accumulate_grad\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    274\u001B[0m )\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Example training loop\n",
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_features, in train_loader:  # DataLoader will unpack the features\n",
    "        # Forward pass\n",
    "        outputs = model(batch_features)\n",
    "        loss = criterion(outputs, batch_features)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T18:32:38.820723Z",
     "start_time": "2023-11-13T18:30:55.631897Z"
    }
   },
   "id": "2b27ccb22bc30335"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T18:32:38.821245Z",
     "start_time": "2023-11-13T18:32:38.820819Z"
    }
   },
   "id": "3ec8f93062f39e4c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
